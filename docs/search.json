[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RAM Codebook",
    "section": "",
    "text": "About the Guide\nThis guide is a collection of ready-to-use packages and scripts designed to support varous tasks, including the development and testing of data collection tools, data cleaning and processing through running high frequency checks and generation of data quality reports. calculation of WFP cooperate indicators and data analysis. it also features features WFP branded visualization and tools to effectively communicate results through reports and dashboards. The guide is build on top of the VAM Resource Center",
    "crumbs": [
      "About the Guide"
    ]
  },
  {
    "objectID": "index.html#brief-overview",
    "href": "index.html#brief-overview",
    "title": "RAM Codebook",
    "section": "Brief Overview",
    "text": "Brief Overview\nSurvey Planning will introduce SurveyDesigner and how to adopt WFP codebook and prepare XLSForms ready for data collection. also it will cover checking your data collection tools prior data collection by filling dumpy data to run preliminary tests. also it will cover accessing MoDa data through the API in order to automate the process of accessing data throught R Studio.\nData Cleaning & Processing will cover data quality checking components including runing high frequency checks on household level data and logging issues in standarized appraoch of replacing old values with new values in order to have clean data. also it will cover running spatial verification checks and generating data quality reports.\nIndicator Calculation will introduce how to compute WFP cooperate indicators without adoping long scripts. WFPIndicators package will provide comprehensive functions for calculating indicators and users can only provide the required paramers.\nData Analysis will provide data analysis functions for generating results tables and running statistical refrence for the computed indicators. also it will show how to speedup the process of generating results from survey data.\nData Visualization will provide WFP branded color plates and charts for visualizing cooperate indicators and as well keeping branded visuals.\nDashboard & Reports will provide markdown reporting templates for rending branded reports that can be quickly shared with auidance. also the package will provide Dashboard templates that can be easily adopted for making information products quickly.",
    "crumbs": [
      "About the Guide"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "RAM Codebook",
    "section": "About the Author",
    "text": "About the Author\ngetting the needed information during disaster and crisis takes time with traditional assessments taking as long as five weeks and often unable to provide immediate information needs - to address this WFP developed the 72-hour rapid assessment approach",
    "crumbs": [
      "About the Guide"
    ]
  },
  {
    "objectID": "chapter_one.html",
    "href": "chapter_one.html",
    "title": "1  Survey Planning",
    "section": "",
    "text": "1.1 SurveyDesigner\nwhen preparing, planning and designing survey, several factors need to be considered. this stage of the survey focuses on clearly identifying the information that you need to collect and how it will be used. by preparing, planning and designing the the survey you’ll be able to produce more acurate and useful insights.\nThis chapter will outline how to prepare and design survey tool with particular focus on mobile data collection. it addresses the challenges of creating a questionnaire and explains how to use SurveyDeisgner and MoDa to develop an electronic data collection tool.\nSurvey Designer is an application that allows users in the field to quickly and easily build standardized assessment and monitoring surveys. All indicators from the Corporate Results Framework (2022-2025) will be automatically available for country offices in the Survey Designer, as well as a variety of other questions and indicators to generate standardized surveys. Survey Designer allows users to export surveys as an XLSForm, or directly publish the surveys in MoDa or Kobo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Planning</span>"
    ]
  },
  {
    "objectID": "chapter_one.html#checking-xslforms",
    "href": "chapter_one.html#checking-xslforms",
    "title": "1  Survey Planning",
    "section": "1.2 Checking XSLForms",
    "text": "1.2 Checking XSLForms",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Planning</span>"
    ]
  },
  {
    "objectID": "chapter_one.html#data-analysis-plan-dap",
    "href": "chapter_one.html#data-analysis-plan-dap",
    "title": "1  Survey Planning",
    "section": "1.3 Data Analysis Plan (DAP)",
    "text": "1.3 Data Analysis Plan (DAP)\nAnalysis plan is a simple table that lists the information to be collected and guide its analysis. DAP should be prepared prior to designing any data collection tool and helps to ensure the efficiency and keep the process and analysis focused.\n\n1.3.1 Literature Review Matrix\nInformation that has been collected by others but relevant to the current assessment or study is considered as secondary data. it could be population and demographic data, socoi-economic/health and nutrition indicators or livelihood systems.\nthe reason why to use secondary data is to save time, form initial hypothesis, enable to focus on potential information gaps, reduce assessment fatigue and enhance cooperation and coordination among main stakeholders.\n\nwhen information gaps cannot be filled by secondary data is advised to be collected on thos missing issues, if deemed possible and feasible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Planning</span>"
    ]
  },
  {
    "objectID": "chapter_one.html#downloading-data-from-moda",
    "href": "chapter_one.html#downloading-data-from-moda",
    "title": "1  Survey Planning",
    "section": "1.4 Downloading Data from MoDa",
    "text": "1.4 Downloading Data from MoDa\nYou can download data from MoDa server through the API using modadownloader package. its prety straight forward and you need to provide your project initial and token information. reproducible example can be found in the below;\n\n# intall connectmoda from cran\n# install.packages(\"connectoModa\")\n\nlibrary(connectoModa)\n\nWarning: package 'connectoModa' was built under R version 4.3.3\n\n# Example usage to fetch users from MoDA\nyour_data &lt;- get_user_moda(form_id = 56597, Token = \"your_token_here\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Planning</span>"
    ]
  },
  {
    "objectID": "chapter_one.html#sampling",
    "href": "chapter_one.html#sampling",
    "title": "1  Survey Planning",
    "section": "1.5 Sampling",
    "text": "1.5 Sampling\nData collection is expensive and time consuming. its is also time consuming for respondents. deciding on sample size is a trade-off between precision and budget and limiting response burden.\n\n1.5.1 Why take a sample?\nsample size calculations tell you how many sampling units you need to include in your survey to get some required level of precision The larger sample the better\n\n\n1.5.2 What are the steps in Sampling Design?\nto estimate sample size you need to know\n\nEstimate of the prevalence of key indicator\nPrecision desired\nLevel of confidence\nExpected response rate\nPopulation\n\n\n\n1.5.3 How do get a good Sample\n ### Simple Random Sample\nSimple random sampling is the most straightforward of the probability sampling methods. it involves the random selection of households from a complete list of all households within the population of interest. simple random sampling has a statistical advantage over other sampling methods and requires a smaller sample size.\nHow do we draw simple random sample?\n\n\n1.5.4 Stratified Random Sample\nHow to form strata? - How do we draw statistical sample? - How do we perform the weights of results that are obtained due to unequal sample size in each stratum? - what is special about proportional probability (PPS) - when do we apply PPS?\n\n\n1.5.5 Systematic Random Sampling\nsystematic sampling shares the same information requirements as simple random sampling. in constant to random selection, this method involves the systematic selection of households from a complete list of all households within the population of interest.\nwhat are random ordered and periodic purpose? - when do we choose systematic sample? - how do we compute the characteristic of systematic sample\n\n\n1.5.6 Cluster Sampling\nhow do we draw cluster sample? - when is a cluster sampling an effective design for obtaining information at minimum cost?\nwhat is a two-stage cluster sample? - when do we use two-stage sampling? - how do we draw 2 stage sample.\n\ncalculate_sample_size &lt;- function(margin_of_error, confidence_level, population_size) {\n  # Validate input parameters\n  if (!is.numeric(margin_of_error) || margin_of_error &lt;= 0 || margin_of_error &gt;= 1) {\n    stop(\"Margin of error must be a numeric value between 0 and 1 (exclusive).\")\n  }\n  if (!is.numeric(confidence_level) || !confidence_level %in% c(0.90, 0.95, 0.99)) {\n    stop(\"Confidence level must be 0.90, 0.95, or 0.99.\")\n  }\n  if (!is.numeric(population_size) || population_size &lt;= 0) {\n    stop(\"Population size must be a positive numeric value.\")\n  }\n\n  # Calculate the Z-score based on the confidence level\n  alpha &lt;- 1 - confidence_level\n  z_score &lt;- qnorm(1 - alpha / 2)\n\n  # Calculate the sample size\n  p &lt;- 0.5 # Assuming maximum variability (50%) for a conservative estimate\n  numerator &lt;- (z_score^2 * p * (1 - p))\n  denominator &lt;- (margin_of_error^2)\n  sample_size_infinite &lt;- numerator / denominator\n\n  # Adjust for finite population size (if population size is not very large)\n  if (population_size &gt; 0) {\n    sample_size &lt;- ceiling((sample_size_infinite * population_size) /\n                              (sample_size_infinite + population_size - 1))\n  } else {\n    sample_size &lt;- ceiling(sample_size_infinite)\n  }\n\n  return(sample_size)\n}\n\n# Example usage:\nmargin_error &lt;- 0.05 # 5% margin of error\nconfidence &lt;- 0.95 # 95% confidence level\npopulation &lt;- 1000 # Population size of 1000\n\nrecommended_n &lt;- calculate_sample_size(margin_error, confidence, population)\ncat(\"Recommended sample size:\", recommended_n, \"\\n\")\n\nRecommended sample size: 278 \n\n# Example with a very large population (effectively infinite):\nlarge_population_n &lt;- calculate_sample_size(0.03, 0.99, 1000000)\ncat(\"Recommended sample size for large population:\", large_population_n, \"\\n\")\n\nRecommended sample size for large population: 1840",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Survey Planning</span>"
    ]
  },
  {
    "objectID": "chapter_two.html",
    "href": "chapter_two.html",
    "title": "2  Data Cleaning & Processing",
    "section": "",
    "text": "2.1 Productivity & Coverage\nin each chapter or part of the book, there will be dedicated package, so please ensure to install pre-requisted packages before runing the codes.\nData processing and cleaning is critical part as it involves high frequency checks and spot check for survey data to flag issues that need to be verified and made changes by keeping reference of alterations made to the original data. There are predetermined quality checks that can be applied to any data and as well the data officer can draft unique checks based on the context. This brings the flexibility of adopting existing spot checks and as well adding your own checking parameters to the process and generate cleaning log book.\nData cleaning is an essential step between data collection and data analysis. Raw data is always imperfect and needs to be prepared for a high quality analysis and overall replicability.\nData quality means having accurate, complete, and reliable information. It’s like making sure the puzzle pieces fit perfectly, so we can trust the picture they create. Good data quality helps us make better decisions and understand what’s happening.\ntopics to work!! to be deleted :) - assessment tracking sheet report - assessment productivity - daily valid surveys",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Processing</span>"
    ]
  },
  {
    "objectID": "chapter_two.html#productivity-coverage",
    "href": "chapter_two.html#productivity-coverage",
    "title": "2  Data Cleaning & Processing",
    "section": "",
    "text": "2.1.1 Survey Coverage\n\nsurvey_tracking &lt;- summarise(group_by(survey_df, ADMIN1Name, ADMIN2Name), count = n())\ncheck_survey &lt;- unique(na.omit(survey_df$today))\nfor(j in check_survey){\n  survey_tracking[, ncol(survey_tracking) +1] &lt;- (survey_df %&gt;% \n                                                    group_by(ADMIN1Name, ADMIN2Name) %&gt;% \n                                                    summarise(val=sum(na.omit(today==j))) %&gt;% \n                                                    arrange(ADMIN1Name, ADMIN2Name)) $val\n  names(survey_tracking)[ncol(survey_tracking)] &lt;- paste0(\"Date:\", j)\n}\n\nnames(survey_tracking)[names(survey_tracking) == 'count'] &lt;- \"Total_Surveys\"\n\n\nknitr::kable(\n  head(survey_tracking, 5), caption = 'Survey Tracking Table',\n  booktabs = TRUE\n)\n\n\nSurvey Tracking Table\n\n\n\n\n\n\n\n\n\n\n\n\nADMIN1Name\nADMIN2Name\nTotal_Surveys\nDate:1743811200\nDate:1743897600\nDate:1743984000\nDate:1744070400\nDate:1744156800\n\n\n\n\nAdmin 1\n1\n36\n8\n6\n11\n4\n7\n\n\nAdmin 1\n2\n24\n3\n3\n8\n6\n4\n\n\nAdmin 1\n3\n31\n1\n4\n15\n5\n6\n\n\nAdmin 1\n4\n31\n2\n5\n15\n6\n3\n\n\nAdmin 1\n5\n42\n5\n9\n14\n7\n7\n\n\n\n\n\n\n\n2.1.2 Enumerator Performance\n\n# summary of productivity\ndaily_productivity &lt;- survey_df %&gt;% \n  arrange(today) %&gt;% \n  group_by(today) %&gt;% \n  summarise(surveydate_N = n())\n\n# enumerator level productivity\nenum_productivity &lt;- survey_df %&gt;% \n  group_by(EnuName) %&gt;% \n  summarise(days_worked = length(unique(today)), total_surveys_done = n(), daily_average = total_surveys_done/days_worked)\n\n\nknitr::kable(\n  head(enum_productivity, 5), caption = 'Enumerator Productivity',\n  booktabs = TRUE\n)\n\n\nEnumerator Productivity\n\n\nEnuName\ndays_worked\ntotal_surveys_done\ndaily_average\n\n\n\n\nEnum 1\n5\n193\n38.6\n\n\nEnum 2\n5\n200\n40.0\n\n\nEnum 3\n5\n193\n38.6\n\n\nEnum 4\n5\n228\n45.6\n\n\nEnum 5\n5\n186\n37.2\n\n\n\n\n\n\nggplot(enum_productivity, aes(x = EnuName, y = total_surveys_done)) +\n  geom_bar(stat = \"identity\") + theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Processing</span>"
    ]
  },
  {
    "objectID": "chapter_two.html#standard-checks",
    "href": "chapter_two.html#standard-checks",
    "title": "2  Data Cleaning & Processing",
    "section": "2.2 Standard Checks",
    "text": "2.2 Standard Checks\n\n2.2.1 Duplicated Surveys\nThis check will help you to identify if there is duplicated record in the data and in most of the case uuid is the unique identifier which is auto generated.\n\ncheck_dup_surveys &lt;- check_duplicate_uuid(data = survey_df)\n\nThe following UUID(s) are duplicated:\n\n\ncharacter(0)\n\n\n\n\n2.2.2 Check Missing Data\nBefore we think about missing data. first lets understand our survey tool as there will be constraints and relevancy logic patterns that will skip some questions if not relevent. after we’re clear from that. we can know anticipate that some questions are mandatory to collect and therefore we need to check how many missing varibles do we have on each. so in case if you see we’re missing 60% or more of key indicator you can check the tool first and than secondly follow with enumerators to undersatnad the pattern and make correction.\n\ncheck_missing_data &lt;- get_na_response_rates(data = survey_df)\n\n\nknitr::kable(\n  head(check_missing_data, 5), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\n\n\nHere is a nice table!\n\n\n\nquestion\nnum_non_response\nperc_non_response\n\n\n\n\nADMIN1Name\nADMIN1Name\n0\n0\n\n\nADMIN2Name\nADMIN2Name\n0\n0\n\n\nEnuName\nEnuName\n0\n0\n\n\nEnuPartner\nEnuPartner\n0\n0\n\n\nEnuSex\nEnuSex\n0\n0\n\n\n\n\n\n\n\n2.2.3 Check Survey Time\n\nsurvey ended before start time?\nsurvey start time before the first day of data collection\nstart time after today’s date\n\nlets check surveys which do not end on the same day as they started.\n\nsubset(subset(survey_df, as_date(survey_df$start) != as_date(survey_df$end), select = c(\"EnuName\", \"start\", \"end\")))\n\n# A tibble: 2 × 3\n  EnuName start                    end                     \n  &lt;chr&gt;   &lt;chr&gt;                    &lt;chr&gt;                   \n1 Enum 5  2025-03-13T11:36:17+0300 2025-04-14T11:25:18+0300\n2 Enum 5  2025-03-13T11:08:41+0300 2025-04-14T11:25:18+0300\n\n\nAgain lets check surveys that show start time earlier than first day of data collection\n\nsubset(subset(survey_df, as.Date(survey_df$start, \"%y/%m/%d\") &lt; as.Date(\"2025-04-06\", \"%y/%m/%d\")), select = c(\"EnuName\", \"start\", \"today\"))\n\n# A tibble: 0 × 3\n# ℹ 3 variables: EnuName &lt;chr&gt;, start &lt;chr&gt;, today &lt;dttm&gt;\n\n\n\ncheck_survey &lt;- cleanR::survey_time(df = survey_df, time_min = 10, time_max = 30) %&gt;% \n  log_sheet(question.name = \"interview_duration\",\n            issue = \" survey filled with less/more time\",\n            action = \"check\")\n\n\nknitr::kable(\n  head(check_survey, 5), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\n\n\nHere is a nice table!\n\n\n\n\n\n\n\n\n\n\n\nuuid\nquestion.name\nissue\nfeedback\naction\nold.value\nnew.value\n\n\n\n\n76219b50-cd45-424044-868483-852c3fe17d49\ninterview_duration\nsurvey filled with less/more time\n\ncheck\n46069.0166666667\n\n\n\n0e4cd3f5-3dc4-434c4e-aca5a7-0fb6c891de5a\ninterview_duration\nsurvey filled with less/more time\n\ncheck\n46096.6166666667\n\n\n\n51cb7382-4280-414348-87808e-6f2490e183ac\ninterview_duration\nsurvey filled with less/more time\n\ncheck\n-121.7\n\n\n\n25a3b7cf-0f15-46454e-8c8382-50d6873ecfa1\ninterview_duration\nsurvey filled with less/more time\n\ncheck\n-10.2833333333333\n\n\n\nd2f90a3e-47a5-464d4c-92989e-57dec9b1fa02\ninterview_duration\nsurvey filled with less/more time\n\ncheck\n-24.9666666666667\n\n\n\n\n\n\n\n\n2.2.4 Check Other Responses\n\n# first make a list of all other columns included in your data\nother_columns &lt;- c(\"RESPRelationHHH_oth\",\n                   \"HHAsstOthCBTRecName_oth\")\n\ncheck_others &lt;- check_other_responses(data = survey_df, other_columns = other_columns)\n\n\nknitr::kable(\n  head(check_others, 5), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\n\n\nHere is a nice table!\n\n\n\n\n\n\n\n\n\n\n\nuuid\nquestion.name\nissue\nfeedback\naction\nold.value\nnew.value\n\n\n\n\n76219b50-cd45-424044-868483-852c3fe17d49\nRESPRelationHHH_oth\nOther response that need to be checked and recoded\n\ntranslate and recode\nmay be relative\n\n\n\n76219b50-cd45-424044-868483-852c3fe17d49\nHHAsstOthCBTRecName_oth\nOther response that need to be checked and recoded\n\ntranslate and recode\nother response\n\n\n\n0e4cd3f5-3dc4-434c4e-aca5a7-0fb6c891de5a\nRESPRelationHHH_oth\nOther response that need to be checked and recoded\n\ntranslate and recode\ndumpy input for spot checks\n\n\n\n0e4cd3f5-3dc4-434c4e-aca5a7-0fb6c891de5a\nHHAsstOthCBTRecName_oth\nOther response that need to be checked and recoded\n\ntranslate and recode\nneed to translate\n\n\n\nd7cfe208-cb21-4b414c-979c9d-fb9c1a456302\nHHAsstOthCBTRecName_oth\nOther response that need to be checked and recoded\n\ntranslate and recode\nlooong narative\n\n\n\n\n\n\n\n\n2.2.5 Check Outliers\nthere are two common ways to detect ourtliers, first using the range of 3 standards devitations from the mean and secondly using the range of 1.5 inter quartile from the 2st and 3rd uartile.\n\ndetect_outliers &lt;- function(data) {\n  # Select only numeric columns\n  numeric_data &lt;- data |&gt;\n    select(where(is.numeric))\n\n  # Pivot to long format to handle all variables uniformly\n  outlier_data &lt;- numeric_data |&gt;\n    pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") |&gt;\n    mutate(\n      log_value = log(value),\n      is_outlier_raw = abs(value - mean(value, na.rm = TRUE)) &gt; 3 * sd(value, na.rm = TRUE),\n      is_outlier_log = abs(log_value - mean(log_value, na.rm = TRUE)) &gt; 3 * sd(log_value, na.rm = TRUE),\n      type = case_when(\n        is_outlier_raw & is_outlier_log ~ \"both\",\n        is_outlier_raw ~ \"raw\",\n        is_outlier_log ~ \"log\",\n        TRUE ~ \"none\"\n      )\n    ) |&gt;\n    filter(type != \"none\") |&gt;\n    select(variable, value, log_value, type)\n\n  return(outlier_data)\n}\n\noutliers_check &lt;- detect_outliers(data = survey_df)\n\n\nknitr::kable(\n  head(outliers_check, 5), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)\n\n\nHere is a nice table!\n\n\nvariable\nvalue\nlog_value\ntype\n\n\n\n\nLcs_em_Migration\n9999\n9.21024\nraw\n\n\nLcs_em_Begged\n9999\n9.21024\nraw\n\n\nLcs_stress_Saving\n9999\n9.21024\nraw\n\n\nLcs_stress_BorrowCash\n9999\n9.21024\nraw\n\n\nLcs_crisis_ProdAssets\n9999\n9.21024\nraw",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Processing</span>"
    ]
  },
  {
    "objectID": "chapter_two.html#specefic-checks",
    "href": "chapter_two.html#specefic-checks",
    "title": "2  Data Cleaning & Processing",
    "section": "2.3 Specefic Checks",
    "text": "2.3 Specefic Checks\nThis is the power house of the concept as it will not be possible to log all issues that need to inspected from data due to the dynamics of different livelihood zones, socio-economic status and other attributes. therefore this specefic checks unit will show you how to first identify and log all issues that need to be addressed/flagged. the below log sheet function topic will guide you how to master the process.\n\n2.3.1 Demographics Checks\n\n\n2.3.2 Food Consumption Check\nThe calculate_fsl_indicators function is a powerful tool for computing essential food security and livelihood (FSL) indicators, including the food consumption score, household dietary diversity, reduced coping strategy, and livelihood coping strategy, from your raw data.\n\nsurvey_df &lt;- calculate_fsl_indicators(data = survey_df,\n                                 # FCS\n                                 FCSStap = \"FCSStap\", \n                                 FCSPulse = \"FCSPulse\", \n                                 FCSPr = \"FCSPr\", \n                                 FCSVeg = \"FCSVeg\", \n                                 FCSFruit = \"FCSFruit\",\n                                 FCSDairy = \"FCSDairy\", \n                                 FCSFat = \"FCSFat\", \n                                 FCSSugar = \"FCSSugar\", \n                                 cutoff = \"Cat28\", \n                                 # rCSI\n                                 rCSILessQlty = \"rCSILessQlty\", \n                                 rCSIBorrow = \"rCSIBorrow\", \n                                 rCSIMealSize = \"rCSIMealSize\", \n                                 rCSIMealAdult = \"rCSIMealAdult\", \n                                 rCSIMealNb = \"rCSIMealNb\",\n                                 # HHS\n                                 HHhSNoFood_FR = \"HHhSNoFood_FR\", \n                                 HHhSBedHung_FR = \"HHhSBedHung_FR\", \n                                 HHhSNotEat_FR = \"HHhSNotEat_FR\", \n                                 # HDDS\n                                 # HDDSStapCer = \"HDDSStapCer\", \n                                 # HDDSStapRoot = \"HDDSStapRoot\", \n                                 # HDDSVeg = \"HDDSVeg\", \n                                 # HDDSFruit = \"HDDSFruit\", \n                                 # HDDSPrMeat = \"HDDSPrMeat\", \n                                 # HDDSPrEgg = \"HDDSPrEgg\", \n                                 # HDDSPrFish = \"HDDSPrFish\", \n                                 # HDDSPulse = \"HDDSPulse\", \n                                 # HDDSDairy = \"HDDSDairy\", \n                                 # HDDSFat = \"HDDSFat\", \n                                 # HDDSSugar = \"HDDSSugar\", \n                                 # HDDSCond = \"HDDSCond\"\n                                 )\n\nBy incorporating ridge charts into your analysis, you can easily identify patterns and variations in FCS and rCSI across different clusters or field monitors.\n\n(plot_ridge_distribution(survey_df, numeric_cols = c(\"FCSStap\", \"FCSPulse\", \"FCSPr\", \"FCSVeg\", \"FCSFruit\", \"FCSDairy\", \"FCSFat\", \"FCSSugar\"),\n                         name_groups = \"Food Groups\", name_units = \"Days\", grouping = \"EnuName\"))\n\n\n\n\n\n\n\n\nBy carefully examining and interpreting the ridge chart, you can gain valuable insights into the distributions and flag any inconsistency for validation and review during data collection. in the below chart, we’ll group the reduced coping strategies at area office level.\n\n(plot_ridge_distribution(survey_df, numeric_cols = c(\"rCSILessQlty\", \"rCSIBorrow\", \"rCSIMealSize\", \"rCSIMealAdult\", \"rCSIMealNb\"),\n                         name_groups = \"Food Coping Strategy\", name_units = \"Days\", grouping = \"EnuName\"))\n\n\n\n\n\n\n\n\nsimilarly, you can group distributions at field monitor level to gain more insight at the consistency of reported distributions across different monitors.\n\n(plot_ridge_distribution(survey_df, numeric_cols = c(\"rCSILessQlty\", \"rCSIBorrow\", \"rCSIMealSize\", \"rCSIMealAdult\", \"rCSIMealNb\"),\n                         name_groups = \"Food Coping Strategy\", name_units = \"Days\", grouping = \"EnuName\"))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Processing</span>"
    ]
  },
  {
    "objectID": "chapter_two.html#apply-cleaning-log",
    "href": "chapter_two.html#apply-cleaning-log",
    "title": "2  Data Cleaning & Processing",
    "section": "2.4 Apply Cleaning Log",
    "text": "2.4 Apply Cleaning Log\nlogbook is the main power-hourse and it will help you flag your data based on the standard and specefic checks and log them in one format and send it to field team for review and feedback and finally use the new value information to replace with the old values in order to generate clean data. using log_sheet() you can achive it all.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Processing</span>"
    ]
  },
  {
    "objectID": "chapter_two.html#spatial-verification",
    "href": "chapter_two.html#spatial-verification",
    "title": "2  Data Cleaning & Processing",
    "section": "2.5 Spatial Verification",
    "text": "2.5 Spatial Verification\nIn order to ensure the accuracy and relablity of data we need to run spatial verification checks that will help us to determine if the data or the point is collected on the right location. for instance, lets assume we’re doing school feeding programme baseline and want to collect 2 surveys from 5 schools in two districts. 10 surveys were than uploaded into the MoDa server after 3 days. and now we need to check if the schools visited are the right sampled ones first, and secondly if the distribution of the data is also aligning with the schools.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Processing</span>"
    ]
  },
  {
    "objectID": "chapter_three.html",
    "href": "chapter_three.html",
    "title": "3  Calculating Indicators",
    "section": "",
    "text": "3.1 Food Security & Essential Needs\nThis chapter will guide you computing outcome indicators in the cooperate indicators compendium. users will only provide data that is inline with the WFP codebook as arguments and than functions will facilitate doing the hard work and do calculations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Calculating Indicators</span>"
    ]
  },
  {
    "objectID": "chapter_three.html#food-security-essential-needs",
    "href": "chapter_three.html#food-security-essential-needs",
    "title": "3  Calculating Indicators",
    "section": "",
    "text": "3.1.1 Food Consumption Score (FCS)\nThe food consumption score (FCS) indicator is a composite score based on households’ dietary diversity, food consumption frequency and relative nutritional value of different food groups. The FCS aggregated household-level food consumption data, in terms of frequency over the previous seven days and weights data accordingly to the relative value of the consumed food groups.\nCut-off thresholds are applied to the FCS to classify households into three groups: poor, borderline or acceptable food consumption.\n\nsurvey_df3 &lt;- compute_fcs(df = survey_df3,\n                          FCSStap = \"FCSStap\",\n                          FCSPr = \"FCSPr\",\n                          FCSPulse = \"FCSPulse\",\n                          FCSVeg = \"FCSVeg\",\n                          FCSFruit = \"FCSFruit\",\n                          FCSDairy = \"FCSDairy\",\n                          FCSFat = \"FCSFat\",\n                          FCSSugar = \"FCSSugar\",\n                          cutoff = \"Cat21\")\n\n\nknitr::kable(\n  head(survey_df3[, c(\"ADMIN1Name\", \"FCS\", \"FCSCat\")], 5),\n  caption = \"Survey Tracking Table\",\n  booktabs = TRUE\n)\n\n\nSurvey Tracking Table\n\n\nADMIN1Name\nFCS\nFCSCat\n\n\n\n\nAdmin 3\n63.5\n3\n\n\nAdmin 1\n68.5\n3\n\n\nAdmin 1\n57.5\n3\n\n\nAdmin 1\n40.5\n3\n\n\nAdmin 3\n38.0\n3\n\n\n\n\n\n\n\n3.1.2 Food Consumption Score Nutrition (FCS-N)\nFCS-N is a measure of household’s adequacy of key macro and micronutrients-rich food groups. In order to assess nutrient inadequacy, FCS-N looks at the frequencies of consumption of protein-rich, Hem Iron and Vitamin A-rich foods over the 7 days prior to the interview.\n\n# first record NA values to 0\nsurvey_df3$FCSNPrMeatF[is.na(survey_df3$FCSNPrMeatF)] &lt;- 0\nsurvey_df3$FCSNPrMeatO[is.na(survey_df3$FCSNPrMeatO)] &lt;- 0\nsurvey_df3$FCSNPrFish[is.na(survey_df3$FCSNPrFish)] &lt;- 0\nsurvey_df3$FCSNPrEggs[is.na(survey_df3$FCSNPrEggs)] &lt;- 0\nsurvey_df3$FCSVeg[is.na(survey_df3$FCSVeg)] &lt;- 0\nsurvey_df3$FCSNVegGre[is.na(survey_df3$FCSNVegGre)] &lt;- 0\nsurvey_df3$FCSFruit[is.na(survey_df3$FCSFruit)] &lt;- 0\n\n# Compute aggregates of key micronutrient consumption\n## Vitamin A-Rich Foods\nsurvey_df3 &lt;- survey_df3 %&gt;% mutate(FGVitA = FCSDairy +FCSNPrMeatO +FCSNPrEggs +\n                                  FCSNVegOrg +FCSNVegGre +FCSNFruiOrg)\nvar_label(survey_df3$FGVitA) &lt;- \"Consumption of vitamin A-rich foods\"\n\n## Protein-Rich Foods\nsurvey_df3 &lt;- survey_df3 %&gt;% mutate(FGProtein = FCSPr +FCSDairy +FCSNPrMeatF +\n                                  FCSNPrMeatO +FCSNPrFish +FCSNVegOrg)\nvar_label(survey_df3$FGProtein) &lt;- \"Consumption of protein-rich foods\"\n\n## Iron-Rich Foods\nsurvey_df3 &lt;- survey_df3 %&gt;% mutate(FGHIron = FCSNPrMeatF +FCSNPrMeatO +FCSNPrFish)\nvar_label(survey_df3$FGHIron) &lt;- \"Consumption of heme iron-rich foods\"\n\n## recode into nutritious groups\nsurvey_df3 &lt;- survey_df3 %&gt;% mutate(FGVitACat = case_when(FGVitA == 0 ~ 1,\n                                                          between(FGVitA,1,6) ~ 2, \n                                                          FGVitA &gt;= 7 ~ 3),\n                                    FGProteinCat = case_when(FGProtein == 0 ~ 1, \n                                                             between(FGProtein,1,6) ~ 2,\n                                                             FGProtein &gt;= 7 ~ 3),\n                                    FGHIronCat = case_when(FGHIron == 0 ~ 1,\n                                                           between(FGHIron,1,6) ~ 2,\n                                                           FGHIron &gt;= 7 ~ 3))\n\n# define variables labels and properties for FGVitACat FGProteinCat FGHIronCat\nsurvey_df3 &lt;- survey_df3 %&gt;%\n  mutate(across(c(FGVitACat, FGProteinCat, FGHIronCat), \n                ~labelled(., labels = c(\n                  \"Never consumed\" = 1,\n                  \"Consumed sometimes\" = 2,\n                  \"Consumed at least 7 times\" = 3\n                ))))\n\nsurvey_df3 &lt;- survey_df3 %&gt;%\n  mutate(across(c(FGVitACat, FGProteinCat, FGHIronCat),\n                ~factor(., levels = c(1, 2, 3),\n                        labels = c(\"Never consumed\", \"Consumed sometimes\", \n                                   \"Consumed at least 7 times\"))))\n\n\nknitr::kable(\n  head(survey_df3[, c(\"ADMIN1Name\", \"FGVitACat\", \"FGProteinCat\", \"FGHIronCat\")], 5),\n  caption = \"FCSN table\",\n  booktabs = TRUE\n)\n\n\nFCSN table\n\n\n\n\n\n\n\n\nADMIN1Name\nFGVitACat\nFGProteinCat\nFGHIronCat\n\n\n\n\nAdmin 3\nConsumed at least 7 times\nConsumed at least 7 times\nConsumed sometimes\n\n\nAdmin 1\nConsumed at least 7 times\nConsumed at least 7 times\nConsumed at least 7 times\n\n\nAdmin 1\nConsumed at least 7 times\nConsumed at least 7 times\nConsumed at least 7 times\n\n\nAdmin 1\nConsumed at least 7 times\nConsumed at least 7 times\nConsumed at least 7 times\n\n\nAdmin 3\nConsumed at least 7 times\nConsumed at least 7 times\nConsumed at least 7 times\n\n\n\n\n\n\n\n3.1.3 Household Dietary Diversity (HDDs)\n\n# compute HHDs\ncompute_hdds &lt;- function(df,\n                         HDDSStapCer,\n                         HDDSStapRoot,\n                         HDDSPulse,\n                         HDDSDairy,\n                         HDDSPrMeat,\n                         HDDSPrFish,\n                         HDDSPrEggs,\n                         HDDSVeg,\n                         HDDSFruit,\n                         HDDSFat,\n                         HDDSSugar,\n                         HDDSCond) {\n  \n  # Convert relevant columns to numeric\n  df &lt;- df %&gt;%\n    mutate(across(\n      .cols = all_of(c(HDDSStapCer, HDDSStapRoot, HDDSPulse, HDDSDairy,\n                       HDDSPrMeat, HDDSPrFish, HDDSPrEggs, HDDSVeg,\n                       HDDSFruit, HDDSFat, HDDSSugar, HDDSCond)),\n      .fns = ~ as.numeric(.),\n      .names = \"num_{.col}\"\n    ))\n  \n  # Compute HDDS using the numeric versions\n  df &lt;- df %&gt;%\n    rowwise() %&gt;%\n    mutate(HDDS = sum(c_across(starts_with(\"num_\")), na.rm = TRUE)) %&gt;%\n    ungroup()\n  \n  # Categorize HDDS\n  df &lt;- df %&gt;%\n    mutate(HDDSCat_IPC = case_when(\n      HDDS &lt;= 2 ~ 1,\n      HDDS &gt;= 3 & HDDS &lt;= 4 ~ 2,\n      HDDS == 5 ~ 3,\n      HDDS &gt;= 6 ~ 4\n    ))\n  \n  return(df)\n}\n\n\nsurvey_df3 &lt;- compute_hdds(df = survey_df3,\n                           HDDSStapCer = \"HDDSStapCer\",\n                           HDDSStapRoot = \"HDDSStapRoot\",\n                           HDDSPulse = \"HDDSPulse\",\n                           HDDSDairy = \"HDDSDairy\",\n                           HDDSPrMeat = \"HDDSPrMeat\",\n                           HDDSPrFish = \"HDDSPrFish\",\n                           HDDSPrEggs = \"HDDSPrEggs\",\n                           HDDSVeg = \"HDDSVeg\",\n                           HDDSFruit = \"HDDSFruit\",\n                           HDDSFat = \"HDDSFat\",\n                           HDDSSugar = \"HDDSSugar\",\n                           HDDSCond = \"HDDSCond\")\n\n\nknitr::kable(\n  head(survey_df3[, c(\"ADMIN1Name\", \"HDDS\", \"HDDSCat_IPC\")], 5),\n  caption = \"Survey Tracking Table\",\n  booktabs = TRUE\n)\n\n\nSurvey Tracking Table\n\n\nADMIN1Name\nHDDS\nHDDSCat_IPC\n\n\n\n\nAdmin 3\n6\n4\n\n\nAdmin 1\n4\n2\n\n\nAdmin 1\n7\n4\n\n\nAdmin 1\n6\n4\n\n\nAdmin 3\n7\n4\n\n\n\n\n\n\n\n3.1.4 Reduced Coping Strategies (rCSI)\nThe Reduced Coping Strategy Index (rCSI), also called CSI food, is used to assess the level of stress3 faced by a household due to a food shortage. It is measured by combining the frequency and severity of the food consumption based strategies households are engaging in. It is calculated using the five standard strategies using a 7-day recall period.\n\nsurvey_df3 &lt;- survey_df3 %&gt;% \n  mutate(rCSI = rCSILessQlty + (rCSIBorrow * 2) + rCSIMealSize + (rCSIMealAdult * 3) + rCSIMealNb)\n\n\nknitr::kable(\n  head(survey_df3[, c(\"ADMIN1Name\", \"rCSI\")], 5),\n  caption = \"rCSI table\",\n  booktabs = TRUE\n)\n\n\nrCSI table\n\n\nADMIN1Name\nrCSI\n\n\n\n\nAdmin 3\n14\n\n\nAdmin 1\n39\n\n\nAdmin 1\n20\n\n\nAdmin 1\n13\n\n\nAdmin 3\n16\n\n\n\n\n\n\n\n3.1.5 Livelihood Coping Strategies (LCS-FS)\nThe livelihoods-based coping strategies module is used to better understand longer-term coping capacity of households. For each country, the module must be adapted to suit each country’s context and poor people’s living conditions.\n\nsurvey_df3 &lt;- survey_df3 %&gt;% \n  mutate(\n    # stress\n    stress_coping_FS = case_when(\n      Lcs_stress_DomAsset == 20 |  Lcs_stress_DomAsset == 30 ~ 1,\n      Lcs_stress_Saving == 20 | Lcs_stress_Saving == 30 ~ 1,\n      Lcs_stress_BorrowCash == 20 | Lcs_stress_BorrowCash == 30 ~ 1,\n      Lcs_stress_Edu == 20 | Lcs_stress_Edu == 30 ~1,\n      TRUE ~ 0\n    ),\n    \n    # crisis\n    crisis_coping_FS = case_when(\n      Lcs_crisis_Health == 20 |  Lcs_crisis_Health == 30 ~ 1,\n      Lcs_crisis_OutSchool == 20 | Lcs_crisis_OutSchool == 30 ~ 1,\n      Lcs_crisis_ProdAssets == 20 | Lcs_crisis_ProdAssets == 30 ~ 1,\n      TRUE ~ 0\n    ),\n    \n    # emergency\n    emergency_coping_FS = case_when(\n      Lcs_em_Migration == 20 |  Lcs_em_Migration == 30 ~ 1,\n      Lcs_em_ResAsset == 20 | Lcs_em_ResAsset == 30 ~ 1,\n      Lcs_em_Begged == 20 | Lcs_em_Begged == 30 ~ 1,\n      TRUE ~ 0\n    )\n    \n  )\n\n\nsurvey_df3 &lt;- survey_df3 %&gt;% mutate(Max_coping_behaviourFS = case_when(\n  emergency_coping_FS == 1 ~ 4,\n  crisis_coping_FS == 1 ~ 3,\n  stress_coping_FS == 1 ~ 2,\n  TRUE ~ 1))\n\nvar_label(survey_df3$Max_coping_behaviourFS) &lt;- \"Summary of asset depletion\"\nval_lab(survey_df3$Max_coping_behaviourFS) = num_lab(\"\n             1 HH not adopting coping strategies\n             2 Stress coping strategies\n             3 Crisis coping strategies\n             4 Emergencies coping strategies\n\")\n\n\nknitr::kable(\n  head(survey_df3[, c(\"ADMIN1Name\", \"Max_coping_behaviourFS\")], 5),\n  caption = \"LCS table\",\n  booktabs = TRUE\n)\n\n\nLCS table\n\n\nADMIN1Name\nMax_coping_behaviourFS\n\n\n\n\nAdmin 3\n4\n\n\nAdmin 1\n4\n\n\nAdmin 1\n4\n\n\nAdmin 1\n4\n\n\nAdmin 3\n4\n\n\n\n\n\n\n\n3.1.6 Household Hunger Scale (HHs)\n\nsurvey_df3 &lt;- survey_df3 %&gt;%\n  mutate(\n    HHhSNoFood_FR_r = case_when(\n      HHSNoFood_FR == \"1\" ~ 1,\n      HHSNoFood_FR == \"2\" ~ 1,\n      HHSNoFood_FR == \"3\" ~ 2,\n      TRUE ~ 0\n    ),\n    HHhSBedHung_FR_r = case_when(\n      HHSBedHung_FR == \"1\" ~ 1,\n      HHSBedHung_FR == \"2\" ~ 1,\n      HHSBedHung_FR == \"3\" ~ 2,\n      TRUE ~ 0\n    ),\n    HHhSNotEat_FR_r = case_when(\n      HHSNotEat_FR == \"1\" ~ 1,\n      HHSNotEat_FR == \"2\" ~ 1,\n      HHSNotEat_FR == \"3\" ~ 2,\n      TRUE ~ 0\n    )\n  )\n\nsurvey_df3 &lt;- survey_df3 %&gt;%\n  mutate(HHS = HHhSNoFood_FR_r + HHhSBedHung_FR_r + HHhSNotEat_FR_r)\n\nsurvey_df3 &lt;- survey_df3 %&gt;%\n  mutate(HHSCat = case_when(\n    HHS %in% c(0,1) ~ \"No or little hunger in the household\",\n    HHS %in% c(2,3) ~ \"Moderate hunger in the household\",\n    HHS &gt;= 4 ~ \"Severe hunger in the household\"\n  ))\n\n\nknitr::kable(\n  head(survey_df3[, c(\"ADMIN1Name\", \"HHS\", \"HHSCat\")], 5),\n  caption = \"rCSI table\",\n  booktabs = TRUE\n)\n\n\nrCSI table\n\n\nADMIN1Name\nHHS\nHHSCat\n\n\n\n\nAdmin 3\n5\nSevere hunger in the household\n\n\nAdmin 1\n4\nSevere hunger in the household\n\n\nAdmin 1\n3\nModerate hunger in the household\n\n\nAdmin 1\n3\nModerate hunger in the household\n\n\nAdmin 3\n5\nSevere hunger in the household\n\n\n\n\n\n\n\n3.1.7 CARI Console",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Calculating Indicators</span>"
    ]
  },
  {
    "objectID": "chapter_three.html#nutrition",
    "href": "chapter_three.html#nutrition",
    "title": "3  Calculating Indicators",
    "section": "3.2 Nutrition",
    "text": "3.2 Nutrition\n\n3.2.1 Minimum Acceptable Deit (MAD)\nMAD is a composite indicator used for assessing Infant and Young Children Feeding (IYCF) among children 6 – 23 months.\n\nMinimum Diet Diversity 6-12 months (MDD): Percentage of children 6–23 months of age who consumed foods and beverages from at least five out of eight defined food groups during the previous day.\nMinimum Meal Frequency 6-23 months (MMF): Percentage of children 6–23 months of age who consumed solid, semi-solid or soft foods (but also including milk feeds for non-breastfed children) at least the minimum number of times during the previous day.\nMinimum Milk Feeding Frequency for Non-Breastfed Children 6-23 months (MMFF): Percentage of non-breastfed children 6–23 months of age who consumed at least two milk feeds during the previous day.\n\n\n\n3.2.2 Minimum Deitary Diversity for Women (MDD-W)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Calculating Indicators</span>"
    ]
  },
  {
    "objectID": "chapter_four.html",
    "href": "chapter_four.html",
    "title": "4  Data Analysis",
    "section": "",
    "text": "4.1 Categorical Analysis\nAnalysis are produced with a purpose and must respect certain imperatives to ensure final usage. this chapter will guide you through running analysis and produce results. You have collected and processed your data and now the fun starts. Data analysis is the practice of applying statistical and analytically tools to a dataset to serve actionable business insights.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_four.html#categorical-analysis",
    "href": "chapter_four.html#categorical-analysis",
    "title": "4  Data Analysis",
    "section": "",
    "text": "4.1.1 Select One\n\n\n4.1.2 Select Multiple\n\nmulti_response_odk_single&lt;-function(data,labelset,id_string=NULL,multicolumn=NULL,other_string=NULL,retain_empty=TRUE,language=\"English\"){\n  \n  if(!paste0(\"label::\",language)%in%colnames(labelset)){\n    \n    newlanguage&lt;-gsub(\"label::\",\"\",colnames(select(labelset,contains(\"label\")))[1])\n    \n    warning(paste(\"language\",language,\"not found. Using first column labelled\",newlanguage,\"instead\"))\n    language&lt;-newlanguage\n  }\n  \n  if((is.null(id_string) & is.null(multicolumn))|(!is.null(id_string) & !is.null(multicolumn))){\n    stop(\"Must include input for id_string or multicolumn\")\n  }\n  \n  if(is.null(multicolumn)){\n    data %&gt;% \n      mutate(group=1) %&gt;%\n      select(X_uuid,group,contains(id_string)) %&gt;%\n      pivot_longer(contains(id_string),names_to = \"value\",values_to = \"exist\") %&gt;%\n      filter(exist==1) %&gt;%\n      mutate(value=str_remove(value,id_string)) %&gt;%\n      select(-exist) %&gt;%\n      full_join(\n        labelset,by=c(\"value\"=\"name\"))-&gt;long_responses\n  }\n  else{\n    \n    data %&gt;% \n      separate(multicolumn,sep=\" \",into=paste0(multicolumn,1:(1+max(str_count(.[,multicolumn],\" \"))))) %&gt;%\n      select(X_uuid,contains(multicolumn)) %&gt;%\n      pivot_longer(contains(multicolumn),names_to = \"selection\",values_to = \"value\",values_drop_na = TRUE) %&gt;%\n      select(-selection) %&gt;%\n      mutate(group=1) %&gt;%\n      full_join(\n        labelset,by=c(\"value\"=\"name\"))-&gt;long_responses\n  }\n  \n  \n  overall_respondents_bygroup &lt;- data  %&gt;% \n    mutate(group=1) %&gt;%\n    group_by(group) %&gt;%\n    summarise(N_respondents=n()) \n  \n  long_responses %&gt;%\n    mutate(count=ifelse(is.na(group),0,1)) %&gt;%\n    mutate(group=1) %&gt;%\n    group_by(group) %&gt;%\n    mutate(N_responses=sum(count)) %&gt;%\n    ungroup() %&gt;%\n    select(value,group,label=contains(paste0(\"label::\",language)),count,N_responses) %&gt;%\n    group_by(value,group,label,N_responses) %&gt;%\n    summarise(n=sum(count)) %&gt;%\n    ungroup() %&gt;%\n    full_join(overall_respondents_bygroup,by=\"group\") %&gt;%\n    mutate(per_responses=n/N_responses,\n           per_respondents=n/N_respondents) %&gt;%\n    ungroup() %&gt;%\n    select(value,label,n,per_responses,per_respondents,N_responses,N_respondents) %&gt;%\n    arrange(desc(per_responses))-&gt;output\n  \n  if(retain_empty==FALSE){\n    output&lt;-filter(output,per_responses&gt;0)\n  }\n  \n  \n  \n  return(output)\n}\n\nmulti_response_odk&lt;-function(data,labelset,id_string=NULL,multicolumn=NULL,group=NULL,other_string=NULL,retain_empty=TRUE,language=\"English\"){\n  \n  if(!paste0(\"label::\",language)%in%colnames(labelset)){\n    \n    newlanguage&lt;-gsub(\"label::\",\"\",colnames(select(labelset,contains(\"label\")))[1])\n    warning(paste(\"language\",language,\"not found. Using first column labelled\",newlanguage,\"instead\"))\n  }\n  \n  if(!is.null(group)){\n    \n    output&lt;-   data %&gt;%\n      split(data[,group],) %&gt;%\n      map(.f=multi_response_odk_single,labelset,id_string,multicolumn,other_string,retain_empty,language) %&gt;%\n      list_rbind(names_to = group) %&gt;%\n      suppressMessages() %&gt;% suppressWarnings()\n    \n    \n  }\n  else{\n    \n    output&lt;- multi_response_odk_single(data,labelset,id_string,multicolumn,other_string,retain_empty,language) %&gt;%\n      suppressMessages() %&gt;% suppressWarnings()\n    \n  }\n  return(output)\n}\n\n\nlabels&lt;-filter(choices,list_name==\"OtherCBTSrc\")\n\n# fixing the bug\nindicators_data$HHAsstOthCBTRecName.100 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.100)\nindicators_data$HHAsstOthCBTRecName.201 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.201)\nindicators_data$HHAsstOthCBTRecName.202 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.202)\nindicators_data$HHAsstOthCBTRecName.203 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.203)\nindicators_data$HHAsstOthCBTRecName.204 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.204)\nindicators_data$HHAsstOthCBTRecName.300 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.300)\nindicators_data$HHAsstOthCBTRecName.999 &lt;- as.character(indicators_data$HHAsstOthCBTRecName.999)\n\nlabels$name &lt;- as.character(labels$name)\n\n\nmulti_response_odk(data=indicators_data,labelset=labels,id_string=\"HHAsstOthCBTRecName.\")\n\n# A tibble: 7 × 7\n  value label          n per_responses per_respondents N_responses N_respondents\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;         &lt;int&gt;\n1 300   Religious…   525         0.148           0.525        3549          1000\n2 999   Other        521         0.147           0.521        3549          1000\n3 201   Family       520         0.147           0.52         3549          1000\n4 203   Community    501         0.141           0.501        3549          1000\n5 204   Community…   497         0.140           0.497        3549          1000\n6 202   Friends      495         0.139           0.495        3549          1000\n7 100   Government   490         0.138           0.49         3549          1000\n\nmulti_response_odk(data=indicators_data,labelset=labels,id_string=\"HHAsstOthCBTRecName.\",retain_empty = FALSE)\n\n# A tibble: 7 × 7\n  value label          n per_responses per_respondents N_responses N_respondents\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;         &lt;int&gt;\n1 300   Religious…   525         0.148           0.525        3549          1000\n2 999   Other        521         0.147           0.521        3549          1000\n3 201   Family       520         0.147           0.52         3549          1000\n4 203   Community    501         0.141           0.501        3549          1000\n5 204   Community…   497         0.140           0.497        3549          1000\n6 202   Friends      495         0.139           0.495        3549          1000\n7 100   Government   490         0.138           0.49         3549          1000\n\n# split it by admin\nmulti_response_odk(data=indicators_data,labelset=labels,id_string=\"HHAsstOthCBTRecName.\",retain_empty = FALSE,group=\"ADMIN1Name\")\n\n# A tibble: 35 × 8\n   ADMIN1Name value label            n per_responses per_respondents N_responses\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 Admin 1    999   Other          119         0.156           0.569         762\n 2 Admin 1    203   Community      113         0.148           0.541         762\n 3 Admin 1    204   Community l…   112         0.147           0.536         762\n 4 Admin 1    300   Religious o…   107         0.140           0.512         762\n 5 Admin 1    201   Family         106         0.139           0.507         762\n 6 Admin 1    202   Friends        103         0.135           0.493         762\n 7 Admin 1    100   Government     102         0.134           0.488         762\n 8 Admin 2    300   Religious o…   121         0.160           0.555         755\n 9 Admin 2    202   Friends        111         0.147           0.509         755\n10 Admin 2    201   Family         108         0.143           0.495         755\n# ℹ 25 more rows\n# ℹ 1 more variable: N_respondents &lt;int&gt;\n\n\n\nmulti_response_odk(data=indicators_data,labelset=labels,multicolumn = \"HHAsstOthCBTRecName\",retain_empty = FALSE,group=\"ADMIN1Name\",language=\"English (en)\") %&gt;%\n  select(ADMIN1Name,Crop=label,n,`Percentage of Respondents`=per_respondents) %&gt;%\n    mutate(`Percentage of Respondents`=scales::percent(`Percentage of Respondents`))\n\n# A tibble: 48 × 4\n   ADMIN1Name Crop                       n `Percentage of Respondents`\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;                      \n 1 Admin 1    &lt;NA&gt;                     762 364.593%                   \n 2 Admin 1    &lt;NA&gt;                     701 335.407%                   \n 3 Admin 1    Other                    118 56.459%                    \n 4 Admin 1    Community                113 54.067%                    \n 5 Admin 1    Community leaders        112 53.589%                    \n 6 Admin 1    Religious organization   107 51.196%                    \n 7 Admin 1    Family                   106 50.718%                    \n 8 Admin 1    Friends                  103 49.282%                    \n 9 Admin 1    Government               102 48.804%                    \n10 Admin 1    &lt;NA&gt;                       1 0.478%                     \n# ℹ 38 more rows\n\n\n\nmulti_response_odk(data=indicators_data,labelset=labels,multicolumn = \"HHAsstOthCBTRecName\",retain_empty = FALSE) %&gt;%\n  #adding some lines so that no answer comes last and other selections are in reversing order with most popular at top\n  mutate(y_axis=reorder(label,n,sum,na.rm=T)) %&gt;%\n    # mutate(y_axis=relevel(y_axis,ref=\"No answer\")) %&gt;%\n#then using geom_col and some standard ggplot functions to make it look a bit prettier\n  ggplot(aes(y=y_axis,x=per_respondents))+\n    geom_col(col=\"black\")+\n      geom_text(aes(label=scales::percent(per_respondents)),hjust=-0.1,size=2)+\n        scale_x_continuous(labels=scales::percent,limits=c(0,0.5))+\n          xlab(\"Percent of Respondents\")+\n            ylab(\"Response\")+\n              ggtitle(\"Which crops do you grow?\",subtitle=\"Multiple Responses Allowed\")+\n                theme_light()\n\nWarning in multi_response_odk(data = indicators_data, labelset = labels, :\nlanguage English not found. Using first column labelled English (en) instead\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\nmulti_response_odk(data=indicators_data,labelset=labels,multicolumn = \"HHAsstOthCBTRecName\",retain_empty = FALSE,group=\"FCSCat\") %&gt;%\n  #adding some lines so that no answer comes last and other selections are in reversing order with most popular at top\n  mutate(y_axis=reorder(label,n,sum,na.rm=T)) %&gt;%\n  mutate(y_axis=relevel(y_axis,ref=\"Other\")) %&gt;%\n  #then using geom_col and some standard ggplot functions to make it look a bit prettier\n  ggplot(aes(y=y_axis,x=per_respondents,fill=FCSCat))+\n  geom_col(col=\"black\")+\n  facet_wrap(~FCSCat,nrow=1)+\n  geom_text(aes(label=scales::percent(per_respondents)),hjust=-0.1,size=2)+\n  scale_x_continuous(labels=scales::percent,limits=c(0,1))+\n  xlab(\"Percent of Respondents\")+\n  ylab(\"Response\")+\n  ggtitle(\"Which crops do you grow?\",subtitle=\"Multiple Responses Allowed\")+\n  theme_light()\n\nWarning in multi_response_odk(data = indicators_data, labelset = labels, :\nlanguage English not found. Using first column labelled English (en) instead\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_text()`).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_four.html#numerical-analysis",
    "href": "chapter_four.html#numerical-analysis",
    "title": "4  Data Analysis",
    "section": "4.2 Numerical Analysis",
    "text": "4.2 Numerical Analysis\n\n4.2.1 Averages\n\n\n4.2.2 Mean/Medium",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_four.html#statistical-significance-tests",
    "href": "chapter_four.html#statistical-significance-tests",
    "title": "4  Data Analysis",
    "section": "4.3 Statistical Significance Tests",
    "text": "4.3 Statistical Significance Tests",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_four.html#survey-weights",
    "href": "chapter_four.html#survey-weights",
    "title": "4  Data Analysis",
    "section": "4.4 Survey Weights",
    "text": "4.4 Survey Weights\nweighing survey respondents are adjusted to better represent the target population. to do so, the weight given each respondent is adjusted to represent the number of similar respondents in the target population.\nweights can be used for different reasons. in most of the cases, we will use",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapter_five.html",
    "href": "chapter_five.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Visualizing Outcome Indicators\nThe Data Visualization Guidelines are designed to support VAM and Monitoring Officers in building clear and effective charts, dashboards and infographics.\nThese guidelines recommends best practices to effectively communicate WFP work on food security to WFP staff, partners and the public. They provide appropriate tools for both data visualizations novice and experts alike, offering tips and resources to create coherent, visually appealing data visualizations with consistent colour palettes, chart elements and types to align with data visualization best practices.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapter_five.html#visualizing-outcome-indicators",
    "href": "chapter_five.html#visualizing-outcome-indicators",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Food Consumption Score\n\nval_lab(indicators_data$FCSCat) = num_lab(\"\n             1 Poor\n             2 Borderline\n             3 Acceptable\n\")\n\nfcscat21_admin1_table_long &lt;- indicators_data %&gt;% \n  group_by(ADMIN1Name_lab = to_factor(ADMIN1Name)) %&gt;%\n  count(FCSCat21_lab = as.character(FCSCat)) %&gt;%\n  mutate(perc = 100 * n / sum(n)) %&gt;%\n  ungroup() %&gt;% select(-n) %&gt;% mutate_if(is.numeric, round, 1) \n\n\n#create a palette of fcs based on wfpthemes palette and set ordering of values\npal_fcs &lt;- wfp_pal(\"pal_stoplight_3pt\", n = 3)\norder_fcs &lt;- c(\"Acceptable\", \"Borderline\", \"Poor\")\npal_fcs &lt;- setNames(pal_fcs, order_fcs)\n\n#and now the graph - option1 - no y axis \nfcscat21_barplot &lt;- fcscat21_admin1_table_long %&gt;% \n  ggplot() +\n  geom_col(\n    aes(x = fct_reorder2(ADMIN1Name_lab,\n                         perc,  \n                         FCSCat21_lab,\n                         \\(x,y) sum(x*(y==\"Acceptable\"))), \n        y = perc,\n        fill = factor(FCSCat21_lab,level=order_fcs)), \n    width = 0.7) +\n  geom_text(aes(x = ADMIN1Name_lab,\n                y = perc,\n                color = factor(FCSCat21_lab,level=order_fcs),\n                label = paste0(perc, \"%\")),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE,\n            size = 10/.pt) +\n  scale_color_manual(\n    values = c(main_white, main_black, main_white)\n  ) +\n  labs(tag = \"Figure 1\",\n       title = \"Household Food Consumption Score (FCS)\",\n       subtitle = \"Percentage of Households per FCS\",\n       caption = \"Source: RAM Survey 2025\"\n  ) +  scale_fill_manual(values = pal_fcs) + theme_wfp(grid = FALSE, axis_text = \"x\", axis = F, axis_title = F) + theme(text = element_text(family = \"sans\"))\n\n\n# \n# theme(text = element_text(family = \"sans\"))\n\n# plot the graph \nfcscat21_barplot\n\n\n\n\n\n\n\n\n\n\n5.1.2 Vitamin-A Rich Foods\n\nindicators_data$ADMIN1Name &lt;- haven::as_factor(indicators_data$ADMIN1Name)\n\n#set ordering\norder_fcsn &lt;- c(\"Consumed at least 7 times\",\"Consumed sometimes\",\"Never consumed\")\npal_fcsn &lt;- setNames(pal_fcsn, order_fcsn )\n\n# create bar-chart for FGVitACat\npercFGVitA_admin1_table_long &lt;- indicators_data %&gt;%\n  group_by(ADMIN1Name_lab = to_factor(ADMIN1Name), FGVitACat_lab = as.character(FGVitACat)) %&gt;% \n  summarize(count = n()) %&gt;%\n  group_by(ADMIN1Name_lab) %&gt;%\n  mutate(perc = round(count/sum(count) * 100, 1))\n\n`summarise()` has grouped output by 'ADMIN1Name_lab'. You can override using\nthe `.groups` argument.\n\npercFGVitA_barplot &lt;- percFGVitA_admin1_table_long %&gt;% \n  ggplot() +\n  geom_col(\n    aes(x = fct_reorder2(ADMIN1Name_lab,\n                         perc,  \n                         FGVitACat_lab,\n                         \\(x,y) sum(x*(y==\"Consumed at least 7 times\"))), \n        y = perc,\n        fill = factor(FGVitACat_lab, level=order_fcsn)), \n    width = 0.7) +\n  geom_text(aes(x = ADMIN1Name_lab,\n                y = perc,\n                color = factor(FGVitACat_lab, level=order_fcsn),\n                label = paste0(perc, \"%\")),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE,\n            size = 10/.pt) +\n  scale_color_manual(\n    values = c(main_white, main_black, main_white)\n  ) +\n  labs(tag = \"Figure\",\n       title = \"Household Food Consumption Nutritional Analysis by Target Group\",\n       subtitle = \"Percentage of Households Consuming Vitamin-A Rich Foods\",\n       caption = \"Source: PMLE Outcome Monitoring, data collected May 2023\"\n  ) +  scale_fill_manual(values = pal_fcsn) + theme_wfp(grid = FALSE, axis_text = \"x\", axis = F, axis_title = F) + theme(text = element_text(family = \"sans\"))\n\npercFGVitA_barplot\n\n\n\n\n\n\n\n\n\n\n5.1.3 Reduced Coping Strategies\n\ndata &lt;- sampledataenglish %&gt;% mutate(rCSI = rCSILessQlty + \n                          (rCSIBorrow * 2) + \n                          rCSIMealNb + \n                          rCSIMealSize + \n                          (rCSIMealAdult * 3))\n\n# Create table of rCSI by ADMIN1 (unweighted) ----------------------------------------------#\n\nrcsi_admin1_table_long &lt;- data %&gt;% \n  mutate(ADMIN1Name_lab = to_factor(ADMIN1Name)) %&gt;% \n  group_by(ADMIN1Name_lab) %&gt;% \n  drop_na(rCSI) %&gt;%   \n  summarise(meanrCSI = round(mean(rCSI),1))\n\n\nrcsi_barplot &lt;- rcsi_admin1_table_long %&gt;% ggplot() +\n  geom_col(aes(\n    x = meanrCSI,\n    y = reorder(ADMIN1Name_lab, meanrCSI),\n  ),\n  fill = wfp_pal(n = 1, \"pal_blue\"),\n  width = 0.8\n  ) +\n  labs(\n    tag = \"Figure 7\",\n    title = \"Reduced Coping Strategy Index (rCSI) by State | April 2023\",\n    subtitle = \"Average rCSI  per Household by State\",\n    x = \"rCSI\",\n    y = \"State\",\n    caption = \"Source: Emergency Food Security Assessment, data collected April 2023\"\n  ) + geom_text(aes(x = meanrCSI,\n                    y = ADMIN1Name_lab, label = meanrCSI),\n                hjust = -0.5,\n                size = 8 / .pt\n  ) +\n  scale_x_continuous(\n    expand = expansion(c(0, 0.1)),\n    breaks = pretty_breaks(n = 7),\n    labels = label_number()\n  ) + theme_wfp(grid = FALSE, axis = \"y\", axis_title = FALSE, axis_text = \"y\") + theme(text = element_text(family = \"sans\"))\n\nrcsi_barplot\n\n\n\n\n\n\n\n\n\n\n5.1.4 Livelihood Coping Strategies\n\nindicators_data &lt;- indicators_data %&gt;% \n  mutate(LhCSI_Category = case_when(\n    Max_coping_behaviourFS == \"Crisis coping strategies\" ~ \"Crisis\",\n    Max_coping_behaviourFS == \"Emergencies coping strategies\" ~ \"Emergency\",\n    Max_coping_behaviourFS == \"Stress coping strategies\" ~ \"Stress\",\n    TRUE ~ \"Neutral\"\n  ))\n\n\n# Calculate the percentage of each level within each region area office\nlcs_admin1_table_long &lt;- indicators_data %&gt;% \n  group_by(ADMIN1Name_lab = to_factor(ADMIN1Name)) %&gt;%\n  count(Max_coping_behaviour_FS_lab = as.character(LhCSI_Category)) %&gt;%\n  mutate(perc = 100 * n / sum(n)) %&gt;%\n  ungroup() %&gt;% select(-n) %&gt;% mutate_if(is.numeric, round, 1) \n\n#this will make sure proper color gets assigned to proper value no mater how table of values was created\norder_lcs &lt;- c(\"Neutral\",\"Stress\",\"Crisis\",\"Emergency\")\npal_lcs &lt;- setNames(pal_lcs, order_lcs)\n\n#and now the graph - option1 - no y axis \nlcs_barplot &lt;- lcs_admin1_table_long %&gt;% \n  ggplot() +\n  geom_col(\n    aes(x = fct_reorder2(ADMIN1Name_lab,\n                         perc,  \n                         Max_coping_behaviour_FS_lab,\n                         \\(x,y) sum(x*(y==\"Neutral\"))), \n        y = perc,\n        fill = factor(Max_coping_behaviour_FS_lab,level=order_lcs)), \n    width = 0.7) +\n  geom_text(aes(x = ADMIN1Name_lab,\n                y = perc,\n                color = factor(Max_coping_behaviour_FS_lab,level=order_lcs),\n                label = paste0(perc, \"%\")),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE,\n            size = 10/.pt) +\n  scale_color_manual(\n    values = c(main_black, main_black, main_white, main_white)\n  ) +\n  labs(tag = \"Figure\",\n       title = \"Household Livelihood Coping Strategies (LCS) by Activity\",\n       subtitle = \"Percentage of Households per LCS groups per Activity\",\n       caption = \"Source: PMLE Outcome Monitoring, data collected May 2023\"\n  ) +  scale_fill_manual(values = pal_lcs) + theme_wfp(grid = FALSE, axis_text = \"x\", axis = F, axis_title = F) +theme(text = element_text(family = \"sans\")) \n\nlcs_barplot",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapter_five.html#chart-types",
    "href": "chapter_five.html#chart-types",
    "title": "5  Data Visualization",
    "section": "5.2 Chart Types",
    "text": "5.2 Chart Types\n\n5.2.1 Bar/Comumn Chart\n\n\n5.2.2 Grouped Bar Chart\n\nabi_summary &lt;- indicators_data %&gt;% \n  group_by(ADMIN1Name_lab = labelled::to_factor(HHHSex)) %&gt;%\n  count(HHSNoFood_lab = labelled::to_factor(ADMIN1Name)) %&gt;%\n  mutate(perc = 100 * n / sum(n)) %&gt;%\n  ungroup() %&gt;% select(-n) %&gt;% mutate_if(is.numeric, round, 1) \n\n\nabi_plot &lt;- ggplot(abi_summary) +geom_bar(\n  aes(x = ADMIN1Name_lab, y = perc, fill = HHSNoFood_lab, group = HHSNoFood_lab), \n  stat='identity', position=position_dodge(.7),  width = 0.6,\n) +\n  geom_text(\n    aes(x = ADMIN1Name_lab, y = perc, label = perc, group = HHSNoFood_lab),\n    position = position_dodge(width = 0.6),\n    vjust = -0.5, size = 2.5\n  )+\n  scale_fill_wfp_b(palette = \"pal_wfp_main\") +\n  labs(\n    title = \"Percentage of Households Reporting Benefit from Assets\",\n    subtitle = \"\",\n    caption = \"Source: PMLE Outcome Monitoring, data collected May 2023\"\n  ) + theme_wfp() +theme(text = element_text(family = \"sans\"))\n\nabi_plot",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapter_six.html",
    "href": "chapter_six.html",
    "title": "6  Reporting & Dashboards",
    "section": "",
    "text": "6.1 Reflections Session\nYou have now done the bigest part of the work and have analyzed data with visualizations. therefdore you’ll need to conduct reflection session by inviting key stakeholders including programme teams and partners to reflect on the preliminary results. at this stage, carefully select the most relevent analysis and charts for interpreatation.in order to keep participants focused, a reflection session shall not last more than 2 hours and include not more than 60 slides",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reporting & Dashboards</span>"
    ]
  },
  {
    "objectID": "chapter_six.html#reflections-session",
    "href": "chapter_six.html#reflections-session",
    "title": "6  Reporting & Dashboards",
    "section": "",
    "text": "6.1.1 Take Reflections Note\n\nReflect: question data quality and make suggestions to adjust and identify additional cleaning steps\nInterpret: develop qualitative intepretations of data patterns\nRecomend: suggest recomendations in terms of programming adjustments\nClasify: define level of sensitivity for certian topics if required",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reporting & Dashboards</span>"
    ]
  },
  {
    "objectID": "chapter_six.html#using-data-library",
    "href": "chapter_six.html#using-data-library",
    "title": "6  Reporting & Dashboards",
    "section": "6.2 Using Data Library",
    "text": "6.2 Using Data Library\nData Library is WFP’s secure space for indexing, versioning and storing data. this service is easy to use and is backed by WFP enterpreice security, management tools, policies and procedures. using the Data Library servies three purposes:\n\nit helps standrize formating of country office (CO) household survey datasets and metadata\nit functions as a secure, presistenty storage for all CO household survey data and as a repository of survey reports\nwhile the data ownership statys with the country office, Data Library enables sharing of specefic relevent datasets and reports with wider audinces such as Regional Bureaus and Headquarters for global analysis and even with parnters and donors for knowledge sharing and accountability.\n\n\n6.2.1 How to Package your Project\nfor each completed assessment and monitoring survey, CO should upload the following informaton in Data Library, each in seperate folder. good practice is to setup your working repository at the planning stage so you can upload each file once completed.\n\nRaw Data\nSntax\nInternal Processed Data\nExternal Processed Data\nOutput Tables\nReports\nTOR and Technical Notes\nQuestionnaire\nSecondary Info\nMiscellaneous\n\n\nThe ambition of RAM is to make household food security and essential needs assessment and monitoring survey data available as a public good",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reporting & Dashboards</span>"
    ]
  }
]